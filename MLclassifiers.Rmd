---
title: "Weather Prediction & Analysis Report"
author: "Zankar Murudkar"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## About the Data and What we need.

Our dataset contains information regarding the weather and atmospheric conditions recorded across various locations in Australia over a long period of time.

**Our Goal** is to predict the possibility of Rainfall the next day based on the pre recorded atmospheric conditions and other factors for the day before. Some of the factors include: Location, Date, Temperatures, Wind Speed and Direction, Humidity, Pressure, etc.

## Data Analysis

Now we will load our dataset and run some scripts to gain a little more insight into the data.

We load our **raw** dataset as **rain** 
```{r load data}
path = "/Users/zankar/Desktop/ML/weatherAUS.csv"
rain <- read.csv(path,header=TRUE,sep=',',stringsAsFactors = F)
```

Checking the dimensions of the dataset.
```{r check dim}
dim(rain)
```
This indicates we have 145460 instances of data and 23 different variables, 22 are independent variables which will be used to predict our dependent Target variable.

Let's find out the variable column names.
```{r colnames}
names(rain)
```
Since we are trying to predict whether we will experience rainfall the next day, our **Target** variable is **RainTomorrow**

Checking the head and tail of data.
```{r headtail}
head(rain)
tail(rain)
```
We have a mix of numerical as well as categorical values. Inspecting from the head and tail, we seem to have significant number of NA values.

## NA Values in data

We check number of NA values present in each column and remove the rows with present NA values.
```{r NAvalues}
sapply(rain, function(x) sum(is.na(x)))
weather <- na.omit(rain)
sapply(weather,function(x) sum(is.na(x)))
```

Our data is now free of NA values. Let's check the dimension, head and tail of the data again.
```{r noNA}
dim(weather)
head(weather)
tail(weather)
```


## Type of Variables

Checking the type of each Column Variable class.
```{r checktype}
str(weather)
```

**Date** is in the year/month/day format, the day and year seem to be irrelevant for our prediction but the month can be a relevant factor. We will extract only the month from the Date column which will result in 12 Categorical values from 1-12 indicating months.
```{r date}
weather$Date <- as.Date(as.character(weather$Date))
weather$Date <- strftime(weather$Date,"%m")
table(weather$Date)
```

As seen earlier with **str()**, we have numerical as well as character values. Character values may be a hindrance while performing numerical operations on the data like calculating distances, correlation, etc.
Therefore we will convert our Character values to numeric.
```{r chartonum}
weather$Location <- as.numeric(as.factor(weather$Location))
weather$WindGustDir <- as.numeric(as.factor(weather$WindGustDir))
weather$WindDir9am <- as.numeric(as.factor(weather$WindDir9am))
weather$WindDir3pm <- as.numeric(as.factor(weather$WindDir3pm))
weather$RainTomorrow <-ifelse(weather$RainTomorrow=='Yes',1,0)
weather$RainToday <-ifelse(weather$RainToday=='Yes',1,0)
weather$Date <- as.numeric(as.factor(weather$Date))
str(weather)
```


## Looking for Constant Variables

Let's check the distribution of values across all our variables.
```{r distribution}
apply(weather,2,table)
```

Distribution of values did not show any constant features, we will still check and remove if we have any constant features.
```{r constant}
isConstant<-function(x) length(names(table(x)))<2
apply(weather,2,isConstant)
```
The result indicates we do not have any constant features.


## Variable Correlation

Multiple Correlated attributes may influence the prediction values. We will check for Correlation and remove attributes which may be highly correlated to any other attributes.

Creating a Correlation matrix between all Predictor attributes.
```{r corr}
cor(weather)
```
Going through the matrix, we can see that Correlation values between MaxTemp,MinTemp,Temp9am, Temp3pm is very high. Similarly we have high Correlation between Pressure9am and Pressure3pm.

Let's plot a graph for the correlation values.
```{r corplot}
if(!require(ggcorrplot))install.packages('ggcorrplot')
library(ggcorrplot)
p<-ggcorrplot(cor(weather))
p + theme(axis.text.x = element_text(angle = 90))
```

Looking for methods to understand Correlation between Predictors efficiently, I came across a very helpful git repository which includes a number of functions for Visualizations and plots.
Below we will use one of the functions from mentioned repository to get top 10-15 correlated variables.
I have commented the install script.
```{r corrplot2}
#devtools::install_github("laresbernardo/lares")
library(lares)
corr_cross(weather, # name of dataset
           max_pvalue = 0.05, # display only significant correlations (at 5% level)
           top = 15 # display top 15 couples of variables (by correlation coefficient)
)
```


## Removing Highly Correlated Variables

From the above plot we can see the 15 most Correlated pairs.
We will subset our dataset and remove Temp3pm, Temp9am and keep MaxTemp and MinTemp only.
Pressure9am and Pressure3pm are the 3rd highest Correlation pair. We will remove Pressure9am and keep the other Predictor.
```{r removecor}
weather2<-subset(weather, select = -c(Temp3pm,Temp9am,Pressure9am))
```

Let's plot the Correlation again to see if we have any other highly correlated pairs.
```{r corrplot3}
p2<-ggcorrplot(cor(weather2))
p2 + theme(axis.text.x = element_text(angle = 90))

corr_cross(weather2, # name of dataset
           max_pvalue = 0.05, # display only significant correlations (at 5% level)
           top = 15 # display top 15 couples of variables (by correlation coefficient)
)
```

We have mostly removed Correlation above 0.8


## RandomForest to determine Important features

We will run the Random Forest algorithm on our data to find the importance of each predictor.

```{r rf}
require(randomForest)
weather2$RainTomorrow<-as.factor(weather2$RainTomorrow)#converts numerical to categorial for rf
class(weather2$RainTomorrow)

weather.rf<-randomForest(RainTomorrow~.,data=weather2)
importance(weather.rf)

require(caret)
varImp(weather.rf)

varImpPlot(weather.rf)
```

Change Target variable back to numeric.
```{r factortonum}
weather2$RainTomorrow<-as.numeric(weather2$RainTomorrow)
weather2$RainTomorrow <-ifelse(weather$RainTomorrow==1,1,0)
```


We will now find the top variables with most importance.

```{r rf2}
importanceOrder=order(-weather.rf$importance)
names=rownames(weather.rf$importance)[importanceOrder][1:15]
names
```

Create a data frame with **Top 15 Predictors** based on Importance.
```{r impdf}
impweather<-weather2[,names]
impweather$RainTomorrow <- weather2$RainTomorrow

dim(impweather)
head(impweather)
```


## Binary or MultiClass Classification ?

```{r biormul}
classLabels<-table(impweather$RainTomorrow)
print(classLabels)
names(classLabels)
length(names(classLabels))
ifelse(length(names(classLabels))==2,"binary classification", "multi-class classification")
```

We have **Binary Classification** at hand.
We will perform **Logistic Regression** and **SVM** which are suitable for Binary Classification along with **Naive Bayes, Multinom & KNN**



## Dividing the data

We now divide the dataset as follows:
10% for Variance Estimation
90% for Generalization, Training and Testing

**For Important Predictors data**
```{r divideimp}
set.seed(43)
varidx<-sample(1:nrow(impweather),0.10*nrow(impweather),replace=F)
impweather2<-impweather[-varidx,]
vardata<-impweather[varidx,]
```

**Dividing data into training and testing data**
70% for training, 30% for testing

```{r trts}
set.seed(43)
tstidx<-sample(1:nrow(impweather2),0.30*nrow(impweather2),replace=F)
trdata<-impweather2[-tstidx,]
tsdata<-impweather2[tstidx,]
```

## Classifiers

We conclude the Exploratory data analysis and use the acquired dataset with minimum correlation between variables and selected important variables.
The dataset will be tested on the below mentioned classifiers:
1) Logistic Regression
2) SVM
3) Naive Bayes
4) KNN
5) Multinomial Logistic Regression

We will find the Test Accuracies from the above mentioned Classifiers and compare them.


## Logistic Regression

Let's start with Logistic Regression.

First setup all the required libraries.
```{r lgr}

library(plotly)
if(!require(stringr))install.packages('stringr')
library(stringr)
if(!require(car))install.packages('car')
library(car)
if(!require(e1071))install.packages('e1071')
library(e1071)
require(e1071)
if(!require(caret))install.packages('caret')
library(caret)
require(caret)
if(!require(pROC))install.packages('pROC')
library(pROC)
if(!require(ROCR))install.packages('ROCR')
library(ROCR)
```


Generalization Phase.
We use the Logistic Regression model on our dataset to find and improve model accuracies.
```{r lgr2}
glm_model<-glm(RainTomorrow~.,data=impweather2, family='binomial')
summary_glm_model<-summary(glm_model)
coef_summary_glm_model<-coef(summary_glm_model)
```

Findind and selecting Predictors with Coefficient values less than 0.05
```{r lgr3}

coef_summary_glm_model
coef_summary_glm_model[,4]<0.05
row_names<-row.names(coef_summary_glm_model[coef_summary_glm_model[,4]<0.05,])
row_names

summary_glm_model$aic
summary_glm_model$null.deviance
summary_glm_model$deviance
ifelse(summary_glm_model$deviance<summary_glm_model$null.deviance,"model has improved","model has not helped")
```

Create a formula string with above found Predictors with Coefficient values less than 0.05
```{r lgr4}
formularhs <- paste(row.names(coef_summary_glm_model[coef_summary_glm_model[,4]<0.05,]),collapse='+')
formularhs

library(stringr)
formularhs <- str_extract(formularhs, "Humidity3pm+(?s)(.*$)")

formulastr<-paste('RainTomorrow~',formularhs,sep='')
formulastr
```

Using the formula string in the glm model.
```{r lgr5}
model2<-glm(formulastr,data=impweather2,family='binomial')
summ_model2<-summary(model2)
coef_summ_model2<-coef(summ_model2)
coef_summ_model2

summ_model2$aic
summ_model2$null.deviance
summ_model2$deviance
```

Running vif to improve the formula string.
```{r lgr6}
library(car)
vif_model<-vif(model2)

vif_model
vif_model[vif_model>4]
names(vif_model)
nl<-names(vif_model[vif_model<4])
(newformulastr<-paste('RainTomorrow~',paste(nl,collapse='+')))
newformulastr
```

Running glm with newly acquired formula string.
```{r lgr7}
newmodel<-glm(newformulastr,data=impweather2,family='binomial')
summnewmodel<-summary(newmodel)
(summnewmodel$aic)
(summnewmodel$deviance)
(summnewmodel$null.deviance)

(p_values<-coef(summnewmodel)[,4])

table(p_values<0.005)
```

Using the glm model to predict Class values and find accuracy.
```{r lgr8}
predYprob<-predict(newmodel,impweather2[,1:15],type='response')
predY<-ifelse(predYprob<0.5,0,1)
table(predY)
table(impweather2[[16]])
table(impweather2[[16]],predY)

library(caret)
library(e1071)

cfm<-caret::confusionMatrix(table(impweather2[[16]],predY))
cfm
```

We now use the above created optimized formula string over training data to create a glm training model.
```{r lgr9}

glm.trmodel<-glm(newformulastr,data=trdata,family='binomial')
summary(glm.trmodel)
predtr<-predict(glm.trmodel,trdata[,1:15],type='response')

predtrclass<-ifelse(predtr<0.5,0,1)
table(trdata[[16]])
table(predtrclass)
levels(factor(predtrclass))
levels(factor(trdata[[16]]))
length(predtrclass)==length(trdata[[16]])
(trcfm<-caret::confusionMatrix(table(trdata[[16]],predtrclass)))
```

The Training accuracies are satisfactory. Therefore we use the training model on the test data.
```{r lgr10}
predts<-predict(glm.trmodel,tsdata[,1:15],type='response')
predtsclass<-ifelse(predts<0.5,0,1)                            
table(predtsclass)
table(tsdata[[16]])
table(tsdata[[16]],predtsclass)
tscfm<-caret::confusionMatrix(table(tsdata[[16]],predtsclass))
tscfm

lgr_tst_accuracy <- tscfm$overall['Accuracy']

(precision <- tscfm$byClass['Pos Pred Value'])    
(recall <- tscfm$byClass['Sensitivity'])
(f_measure <- 2 * ((precision * recall) / (precision + recall))) #geometric mean instead of arithmatic mean

```

Plotting the ROC curve.
```{r lgr11}
graphics.off()
par("mar")
par(mar=c(1,1,1,1))

par(pty="s")
glmROC <- roc(tsdata[[16]]~ predtsclass,plot=TRUE,
              print.auc=TRUE,col="green",lwd =4,
              legacy.axes=TRUE,main="ROC Curves")


getMetrics<-function(actual_class,predicted_response)
{
  X=list()
  if ( require(ROCR) ) {
    auc_1=prediction(predicted_response,actual_class)
    prf=performance(auc_1, measure="tpr",x.measure="fpr")
    slot_fp=slot(auc_1,"fp")
    slot_tp=slot(auc_1,"tp")
    
    fpr=unlist(slot_fp)/unlist(slot(auc_1,"n.neg"))
    tpr=unlist(slot_tp)/unlist(slot(auc_1,"n.pos"))
    
    auc<-performance(auc_1,"auc")
    AUC<-auc@y.values[[1]]
    X=list(fpr=fpr,tpr=tpr,auc=AUC)
  }
  X
}

L<-getMetrics(tsdata[[16]],predts)
plot(L$fpr,L$tpr,main=" ROC Plot tpr vs fpr")
print(paste("AUC=",L$auc,sep=''))
text(paste("AUC=",L$auc,sep=''),x=0.6,y=0.30)


```


## SVM

We use SVM from "e1071" library.
Setting up the required libraries.
```{r svm}

library(plotly)
library(e1071)
library(lattice)
library(caret)
library(pROC)
```

Generalization phase for SVM over dataset.
```{r svm2}
impweather2$RainTomorrow <- factor(impweather2$RainTomorrow)
svm_model <- svm(RainTomorrow~.,data=impweather2,probability=TRUE)
svm_model

svmpredict <- predict(svm_model,impweather2[,-c(16)],probability = TRUE)
probs <- attr(svmpredict,"prob")[,"1"]
predclass <- ifelse(probs>0.5,1,0)
table(impweather2$RainTomorrow==predclass)
(cfmx <- caret::confusionMatrix(table(impweather2$RainTomorrow,predclass)))
```


We now Train the SVM model over training data and find Training error and accuracies.
```{r svm3}

svm_train_model <- svm(RainTomorrow~.,data=trdata,probability=TRUE)

svm_train_predict <- predict(svm_train_model,trdata[,-c(16)],probability = TRUE)
#train_probs <- attr(svm_train_predict,"prob")[,"1"]

train_predclass<-ifelse(svm_train_predict<0.5,0,1)

#train_predclass <- ifelse(train_probs>0.5,1,0)
tr_tbl <- table(trdata$RainTomorrow,train_predclass)
(tr_cfmx <- caret::confusionMatrix(tr_tbl))

cdf <- data.frame(all=cfmx$byClass,tr=tr_cfmx$byClass)
cdf
```

The Training accuracies seem to be satisfactory. Therefore we use the Training model over the test data.
```{r svm4}
svm_tst_predict <- predict(svm_train_model,tsdata[,-c(16)],probability = TRUE)
#tst_probs <- attr(svm_tst_predict,"prob")[,"1"]
tst_predclass <- ifelse(svm_tst_predict>0.5,1,0)
tst_tbl <- table(tsdata$RainTomorrow,tst_predclass)
(tst_cfmx <- caret::confusionMatrix(tst_tbl))

tst_cfmx
svm_tst_accuracy <- tst_cfmx$overall['Accuracy']

cdf <- cbind(cdf,ts=tst_cfmx$byClass)
cdf
```

Plotting ROC curve for SVM. 
```{r svm5}

svm_roc_obj <- roc(tsdata[,c(16)],svm_tst_predict)
svm_auc <- auc(svm_roc_obj)

plot.new()
plot.window(xlim = c(1,0),ylim = c(0,1),xaxs='i',yaxs='i')
axis(1,las=1)
axis(2,las=1)
abline(1,-1,col="black",lwd=0.5)
box()
title(main="ROC",xlab = "Specificity",ylab = "Sensitivity")
lines(svm_roc_obj,col="black",las=2)
text(x=0.2,y=0.2,paste("SVM_AUC=",round(svm_auc,6),sep = ''))


```


Change Target variable back to numeric.

```{r factortonum2}
impweather2$RainTomorrow<-as.numeric(impweather2$RainTomorrow)
str(impweather2$RainTomorrow)
table(impweather2$RainTomorrow)
impweather2$RainTomorrow <-ifelse(impweather2$RainTomorrow==2,1,0)
table(impweather2$RainTomorrow)
```


## Naive Bayes

Setting up required libraries.
```{r nb}
library(e1071)
library(caret)
library(magrittr)
library(dplyr)
```

Generalization phase for NaiveBayes with the entire data.
```{r nb2}
nbtr.model1<-naiveBayes(RainTomorrow~.,data=impweather2)
nbtr.model1
nbtr.pred<-predict(nbtr.model1,impweather2[,-c(16)],type='raw')
nbtr.class<-unlist(apply(round(nbtr.pred),1,which.max))-1
nbtr.tbl<-table(impweather2[[16]], nbtr.class)
cfm<-caret::confusionMatrix(nbtr.tbl)
cfm
```

Checking dimension for training and test data.
```{r nb3}

dim(trdata)
table(trdata$RainTomorrow)
dim(tsdata)
table(tsdata$RainTomorrow)
```

Training the model over Training data and find resulting Training accuracies.
```{r nb4}
nbtr.model<-naiveBayes(RainTomorrow~.,data=trdata)

nbtr.trpred<-predict(nbtr.model,trdata[,-c(16)],type='raw')
nbtr.trclass<-unlist(apply(round(nbtr.trpred),1,which.max))-1
nbtr.trtbl<-table(trdata[[16]], nbtr.trclass)
tr.cfm<-caret::confusionMatrix(nbtr.trtbl)
tr.cfm
```

Using the Training model over test data to find Test accuracies.
```{r nb5}
nbtr.tspred<-predict(nbtr.model,tsdata[,-c(16)],type='raw')

roc.nbtr.tspred<-nbtr.tspred[,2]
nbtr.tsclass<-unlist(apply(round(nbtr.tspred),1,which.max))-1
nbtr.tstbl<-table(tsdata[[16]], nbtr.tsclass)
tst.cfm<-caret::confusionMatrix(nbtr.tstbl)
tst.cfm

nb_tst_accuracy <- tst.cfm$overall['Accuracy']
```


## KNN

We implement KNN from the "class" library. 
Setting up required libraries.
```{r knn}

library(class)
library(ROCR)
```

We set the dataframe, labelcolumn which we wish to predict. 
```{r knn2}

knn.df<-impweather2
knn.ids <- sample(1:nrow(knn.df),nrow(knn.df),replace = F)
knn.df <- knn.df[knn.ids,]
labelcol <- 16
```

Dividing the dataframe into test and training dataframes.
```{r knn3}
set.seed(43)
n<-nrow(knn.df)
knn.df<- knn.df[sample(n),]

train.df <- knn.df[1:as.integer(0.7*n),]
test.df <- knn.df[as.integer(0.7*n +1):n,]
```

Since KNN calculates distances, we convert our labelcolumn to factor.
```{r knn4}
cl<-factor(train.df[,16])
```

Running the KNN classifier with K=12,
We tried the classifier with diifferent values of K from K=4 to K=15, and K=12 seems to provide with consistent accuracy.

```{r knn5}

knnPred<-knn(train.df[,-c(labelcol)],test.df[,-c(labelcol)], cl, k = 12, prob=TRUE)
prob<-attr(knnPred,"prob")
kp<-prediction(prob,test.df[,labelcol])
AUC<-performance(kp,"auc")
#perf <- performance(pred,"tpr","fpr")
#plot(perf,colorize=TRUE)

pred_knn<-performance(kp,"tpr","fpr")
```

Plotting the AUC curve.

```{r knn6}
plot(pred_knn,avg="threshold",colorize=T,lwd=3,main="ROCR knn")
text(0.8,0.2,paste("AUC=",round(AUC@y.values[[1]],4),sep=''))
TBL<-table(test.df[,labelcol],knnPred)
print(paste("accuracy=",sum(diag(TBL))/sum(TBL)))

knn_accuracy <- sum(diag(TBL))/sum(TBL)
```


## Multinomial Logistic Regression

We will run the multinom function from nnet library.

```{r mn}
library(nnet)

multinom_model<-multinom("RainTomorrow~.",data=impweather2)
multinom_model
```

The model is ready for the generalization phase. Now we test our model by predicting class values and comparing with actual values.
```{r mn2}
multinom_pred<-predict(multinom_model,impweather2[,-16],type="class")
mncfm<-caret::confusionMatrix(table(impweather2[,16],multinom_pred))
mncfm
```

Above mentioned are the results for Generalization phase. The model seems to work satisfactorily.
Now we run the multinom model on training data and check training accuracy.

```{r mn3}
multinom_trmodel<-multinom("RainTomorrow~.",data=trdata)

multinom_trpred<-predict(multinom_trmodel,trdata[,-16],type="class")
mntrcfm<-caret::confusionMatrix(table(trdata[,16],multinom_trpred))
mntrcfm
```

We now use the model created using training data on our test data.
```{r mn4}
multinom_tspred<-predict(multinom_trmodel,tsdata[,-16],type="class")
mntscfm<-caret::confusionMatrix(table(tsdata[,16],multinom_tspred))
```

Accuracy measures for Multinom are as follows
```{r mn5}
mntscfm

mn_tst_accuracy <- mntscfm$overall['Accuracy']

```


## Plotting the Accuracies obtained from different classifiers.

We have accuracies obtained from the test data evaluation. The Classifiers used include Logistic Regression, SVM, Naive Bayes, KNN and Multinomial Logistic Regression.
The below plot shows their accuracy comparison.

```{r plotacc}

accuracy_values <- c(lgr_tst_accuracy,svm_tst_accuracy,nb_tst_accuracy,knn_accuracy,mn_tst_accuracy)
accuracy_values <- round(accuracy_values,4)
names(accuracy_values) <- c("Logistic","SVM","NaiveBayes","KNN","Multinom")

ylim <- c(0, 1.1*max(accuracy_values))
## Plot, and store x-coordinates of bars in xx
xx <- barplot(accuracy_values, xaxt = 'n', xlab = '', width = 0.85, ylim = ylim,
              main = "Accuracies for different Classifiers", 
              ylab = "Accuracy")
## Add text at top of bars
text(x = xx, y = accuracy_values, label = accuracy_values, pos = 3, cex = 0.8, col = "red")
## Add x-axis labels 
axis(1, at=xx, labels=names(accuracy_values), tick=FALSE, las=2, line=-0.5)

```


## Variance Estimation

Finding variance of different Claasifier models by finding the test accuracy over data never seen before.
We run the model over 100 times and collect accuracies each time. We then find the variance in found accuracies.
Classifier models used are:
1)SVM
2)NaiveBayes
3)KNN
4)Multinom

We use the invisible() to hide unnecessary computation values generated while running the model over 100 times.

```{r varest,message=FALSE,error=FALSE,warning=FALSE,results='hide'}
invisible({capture.output({

library(nnet)
library(e1071)
library(class)
library(caret)
library(magrittr)
library(dplyr)
library(class)



varianceEstimator = function(tr,ts,percent,classifier){
  
  targetid = which(names(tr)=="RainTomorrow")
  var_accuracy=c();
  
  
  for(i in 1:100){
    var_trid=sample(1:nrow(tr),percent/100*nrow(tr),replace=F)
    var_tr=tr[var_trid,]
    if(classifier=="SVM"){
      
      svm_model_var = svm(RainTomorrow~.,data=var_tr,probability=TRUE)
      pred_var = predict(svm_model_var,ts[,-c(16)],probability = TRUE)
      
      
    }else if(classifier=="NaiveBayes"){
      
      nb_model_var = multinom("RainTomorrow~.",data=var_tr)
      pred_var = predict(multinom_model,ts[,-16],type="class")
      
    }else if(classifier=="KNN"){
      
      labelcol <- 16
      cl<-factor(var_tr[,16])
      pred_var<-knn(var_tr[,-c(labelcol)],ts[,-c(labelcol)], cl, k = 12, prob=TRUE)
      
    }else if(classifier=="Multinom"){
      
      mn_model_var<-multinom("RainTomorrow~.",data=var_tr)
      pred_var<-predict(mn_model_var,ts[,-16],type="class")
      
    }else{
      print("Enter valid classifier")
    }
    
    u_var=union(pred_var,ts[,targetid])
    t_var=table(factor(pred_var,u_var),factor(ts[,targetid],u_var))
    cfm_var=caret::confusionMatrix(t_var)
    cfm_var_accuracy=cfm_var$overall[["Accuracy"]]
    
    var_accuracy=c(var_accuracy,cfm_var_accuracy)
  }
  
  mean_var=signif(mean(var_accuracy),4)
  var_var=signif(var(var_accuracy),4)
  var=data.frame(mean_var,var_var)
  names(var)=c("Accuracy mean","Accuracy variance")
  return(t(var))
}

#Classifier = "SVM" or "NaiveBayes" or "KNN" or "Multinom"

#Testing function for Multinom
var_est_20 = varianceEstimator(impweather2,vardata,20,classifier = "Multinom")
var_est_30 = varianceEstimator(impweather2,vardata,30,classifier = "Multinom")
var_est_50 = varianceEstimator(impweather2,vardata,50,classifier = "Multinom")


})})
```

```{r varest2}
print("Accuracy mean and variance for 20 percent training data")
var_est_20

print("Accuracy mean and variance for 30 percent training data")
var_est_30

print("Accuracy mean and variance for 50 percent training data")
var_est_50

```
